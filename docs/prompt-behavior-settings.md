# Prompt Behavior Settings in AiderDesk

AiderDesk provides several settings to control the behavior of the AI model when generating responses. These settings allow you to fine-tune how prompts are processed, influencing the creativity, length, and determinism of the output. These settings are typically found in the AiderDesk **Settings** menu, often within sections related to "Model," "AI," or specific chat modes.

## Key Prompt Behavior Settings

While the exact names and availability of settings can vary based on the AiderDesk version and the underlying AI model being used, common parameters include:

### 1. Temperature

-   **What it is:** Temperature controls the randomness of the AI's output.
-   **How it works:**
    -   Lower values (e.g., 0.0 to 0.3) make the output more deterministic and focused. The AI will choose the most likely words, leading to more predictable and conservative responses. This is useful for tasks requiring factual accuracy or specific formatting.
    -   Higher values (e.g., 0.7 to 1.0 or higher) make the output more random and creative. The AI might explore less common word choices, leading to more diverse, imaginative, or even unexpected responses. This can be beneficial for brainstorming, content generation, or when seeking novel ideas.
-   **Typical Range:** Usually between 0.0 and 1.0, though some models might support higher ranges.
-   **Use Cases:**
    -   **Low Temperature:** Code generation where specific syntax is required, factual Q&A, summarization.
    -   **High Temperature:** Brainstorming, creative writing, generating multiple diverse options.

### 2. Max Tokens (Max Output Tokens / Max Length)

-   **What it is:** This setting defines the maximum number of tokens (words or pieces of words) that the AI model can generate in a single response.
-   **How it works:** It acts as a hard limit on the length of the AI's output. If the AI needs more tokens to complete its thought or answer fully, it will be cut off once this limit is reached.
-   **Considerations:**
    -   Setting it too low might result in incomplete or truncated responses.
    -   Setting it too high might lead to overly verbose responses and increased token usage (which can affect cost).
    -   The definition of a "token" can vary between models but generally corresponds to a word or a common sub-word unit.
-   **Use Cases:**
    -   **Shorter responses:** Setting a lower limit for quick answers or summaries.
    -   **Longer, detailed responses:** Setting a higher limit for comprehensive explanations, code generation, or document drafting.

### 3. Top P (Nucleus Sampling)

-   **What it is:** Top P, or nucleus sampling, is an alternative to temperature for controlling randomness. Instead of considering all possible next words, it considers only the smallest set of words whose cumulative probability exceeds a certain threshold (P).
-   **How it works:**
    -   A Top P value of 0.1 means the AI will only consider words from the top 10% most probable choices.
    -   A Top P value of 0.9 means the AI will consider words from the top 90% most probable choices, allowing for more diversity.
-   **Relationship with Temperature:** Often, you might use either Temperature or Top P, but not both simultaneously, as they achieve similar goals in different ways. Some interfaces might disable one if the other is actively being adjusted.
-   **Typical Range:** Between 0.0 and 1.0.
-   **Use Cases:** Similar to temperature, but some users find Top P gives more fine-grained control over the balance between coherence and creativity.

### 4. Presence Penalty

-   **What it is:** This setting discourages the AI from repeating tokens (words/phrases) that have already appeared in the generated text.
-   **How it works:** Positive values increase the penalty for repeating tokens, making the AI more likely to introduce new concepts or vocabulary.
-   **Typical Range:** Often between -2.0 and 2.0. A value of 0 means no penalty.
-   **Use Cases:** Useful for generating text that needs to be diverse and avoid repetitiveness, such as creative writing or brainstorming.

### 5. Frequency Penalty

-   **What it is:** Similar to presence penalty, but it penalizes tokens based on how frequently they have already appeared in the generated text (and sometimes in the prompt as well).
-   **How it works:** Higher values make the AI less likely to reuse frequent tokens, encouraging it to use a wider range of vocabulary.
-   **Typical Range:** Often between -2.0 and 2.0. A value of 0 means no penalty.
-   **Use Cases:** Helps in reducing monotonous repetition of common words or phrases, leading to more engaging and varied text.

### 6. Stop Sequences

-   **What it is:** You can define one or more sequences of characters (e.g., a specific phrase, newline character, or a custom token) that, when generated by the AI, will cause it to stop producing further output.
-   **How it works:** If the AI generates a string that matches a stop sequence, the generation process halts at that point, even if the `Max Tokens` limit hasn't been reached.
-   **Use Cases:**
    -   Ensuring the AI stops after a specific section (e.g., after generating a function definition).
    -   Formatting output, like stopping before a known footer or signature.
    -   Preventing the AI from generating irrelevant content after completing the core request.

## Model-Specific Parameters

Some AI models might have unique parameters not listed above. AiderDesk may expose these if they are relevant and configurable through the model's API. Always refer to the specific documentation for the AI model you are using through AiderDesk for a complete list of tunable parameters.

## Where to Find These Settings

-   **Global Settings:** AiderDesk might have a general settings panel where default values for these parameters can be set.
-   **Profile/Mode Settings:** Different chat modes (e.g., "Code," "Agent," "Ask") or agent profiles might have their own customizable sets of these parameters, allowing you to optimize behavior for specific types of tasks.
-   **Per-Prompt (Less Common):** Some interfaces might allow overriding these settings for individual prompts, though this is less common in integrated applications like AiderDesk.

## Best Practices

-   **Experiment:** The optimal settings can vary greatly depending on the task, the AI model, and your desired output style. Don't hesitate to experiment with different values.
-   **Start with Defaults:** Begin with the default settings provided by AiderDesk or the model provider, and then adjust them incrementally.
-   **Consider the Task:**
    -   For creative tasks, try increasing `Temperature` or `Top P`.
    -   For factual or code-related tasks, try decreasing `Temperature`.
    -   Adjust `Max Tokens` based on the expected length of the response.
-   **Take Notes:** When you find a combination of settings that works well for a particular type of task, note it down for future reference.

By understanding and utilizing these prompt behavior settings, you can gain more control over the AI's output in AiderDesk, tailoring its responses to better suit your specific needs and preferences.
